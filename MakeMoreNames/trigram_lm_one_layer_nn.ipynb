{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [name.strip() for name in open('./names.txt', 'r').readlines()]\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctoi = {}\n",
    "itoc = {}\n",
    "chars = set()\n",
    "for name in names:\n",
    "    for c in name:\n",
    "        chars.add(c)\n",
    "\n",
    "for i, c in enumerate(sorted(chars)):\n",
    "    ctoi[c] = i\n",
    "    itoc[i] = c\n",
    "\n",
    "ctoi['.'] = 26\n",
    "itoc[26] = '.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(ctoi.keys())\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram model - given 2 tokens predict the next token\n",
    "@ character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . e\n",
      ". e m\n",
      "e m m\n",
      "m m a\n",
      "m a .\n",
      ". . o\n",
      ". o l\n",
      "o l i\n",
      "l i v\n",
      "i v i\n",
      "v i a\n",
      "i a .\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "for name in names[:2]:\n",
    "    name = '..' + name + '.'\n",
    "    for c1, c2, c3 in zip(name, name[1:], name[2:]):\n",
    "        print(c1,c2,c3)\n",
    "        X.append([ctoi[c1],ctoi[c2]+vocab_size])\n",
    "        y.append(ctoi[c3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 27])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.zeros((len(X), 2*vocab_size))\n",
    "for i in range(len(X)):\n",
    "    input[i][X[i][0]] = 0.5\n",
    "    input[i][X[i][1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'total = len(y)\\ntrain_size = int(total*0.8)\\ndev_size = int(0.5*(total - train_size))\\n# test_size = total - train_size - dev_size # remaining\\nxtrain, xdev, xtest = X[:train_size], torch.tensor(X[train_size:train_size+dev_size]), torch.tensor(X[train_size+dev_size:])\\nytrain, ydev, ytest = F.one_hot(torch.tensor(y[:train_size]), vocab_size), F.one_hot(torch.tensor(y[train_size:train_size+dev_size]), vocab_size), F.one_hot(torch.tensor(y[train_size+dev_size:]), vocab_size)'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not required here\n",
    "\"\"\"total = len(y)\n",
    "train_size = int(total*0.8)\n",
    "dev_size = int(0.5*(total - train_size))\n",
    "# test_size = total - train_size - dev_size # remaining\n",
    "xtrain, xdev, xtest = X[:train_size], torch.tensor(X[train_size:train_size+dev_size]), torch.tensor(X[train_size+dev_size:])\n",
    "ytrain, ydev, ytest = F.one_hot(torch.tensor(y[:train_size]), vocab_size), F.one_hot(torch.tensor(y[train_size:train_size+dev_size]), vocab_size), F.one_hot(torch.tensor(y[train_size+dev_size:]), vocab_size)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN with 2 inputs indices of the characters in the vocabulary\n",
    "# no hidden layer\n",
    "# single output layer of vocab_size, here the input order won't matter since we take 1x27 vector with 2 indices turned on which would be symmetric for m,n or n,m\n",
    "# to add positional sense, add 0 for 1st char, 1 for 2nd and divide entire input vector by 2\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W1 = torch.randn((2*vocab_size, vocab_size), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 27])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "logits = input @ W1\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- probs.shape (12, 27) 12 examples with 27 probabilities indicating probs of each character in vocab\n",
    "- here loss is nothing but the negative log likelihood of the probability of predicting y[i] given the bigram in X[i]\n",
    "- why negative log likelihood? Check out perplexity and entropy relation with negative log likelihood\n",
    "- we want to minimize the negative log-likelihood means we want to maximize the log-likelihood (ll) means we want to maximize the probability of the next character given the input [bigram here]\n",
    "- higher the confidence on next character better the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6864)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_one_example = -probs[0, y[0]].log()\n",
    "# loss over all examples in mini_batch = average of each nll\n",
    "loss = -probs[torch.arange(len(input)), y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for backward pass we need gradients and torch sets require gradients to False by default\n",
    "- hence re-write everything from start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((2*vocab_size, vocab_size), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "logits = input @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "loss = -probs[torch.arange(len(input)), y].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6399, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "W.grad = None # first set gradients to 0 as they are accumulated over time; recall micrograd +=; :)\n",
    "loss.backward() # compute the gradients starting from loss all the way backward to the weights in 1st layer; in micrograd we had implemented a toposort to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54, 27])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the weights\n",
    "learning_rate = 0.1\n",
    "W.data += -learning_rate*W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iterations: 3.639911651611328\n",
      "Loss after 1 iterations: 3.6283226013183594\n",
      "Loss after 2 iterations: 3.6167495250701904\n",
      "Loss after 3 iterations: 3.605191469192505\n",
      "Loss after 4 iterations: 3.59364914894104\n",
      "Loss after 5 iterations: 3.5821235179901123\n",
      "Loss after 6 iterations: 3.570613145828247\n",
      "Loss after 7 iterations: 3.559119939804077\n",
      "Loss after 8 iterations: 3.5476415157318115\n",
      "Loss after 9 iterations: 3.536179780960083\n",
      "Loss after 10 iterations: 3.524733781814575\n",
      "Loss after 11 iterations: 3.5133044719696045\n",
      "Loss after 12 iterations: 3.501891851425171\n",
      "Loss after 13 iterations: 3.4904944896698\n",
      "Loss after 14 iterations: 3.479114532470703\n",
      "Loss after 15 iterations: 3.467749834060669\n",
      "Loss after 16 iterations: 3.4564027786254883\n",
      "Loss after 17 iterations: 3.4450721740722656\n",
      "Loss after 18 iterations: 3.4337587356567383\n",
      "Loss after 19 iterations: 3.422461748123169\n",
      "Loss after 20 iterations: 3.4111814498901367\n",
      "Loss after 21 iterations: 3.3999178409576416\n",
      "Loss after 22 iterations: 3.388671636581421\n",
      "Loss after 23 iterations: 3.3774425983428955\n",
      "Loss after 24 iterations: 3.3662307262420654\n",
      "Loss after 25 iterations: 3.3550357818603516\n",
      "Loss after 26 iterations: 3.3438589572906494\n",
      "Loss after 27 iterations: 3.3326988220214844\n",
      "Loss after 28 iterations: 3.321556806564331\n",
      "Loss after 29 iterations: 3.310431718826294\n",
      "Loss after 30 iterations: 3.2993252277374268\n",
      "Loss after 31 iterations: 3.288235664367676\n",
      "Loss after 32 iterations: 3.2771646976470947\n",
      "Loss after 33 iterations: 3.2661116123199463\n",
      "Loss after 34 iterations: 3.2550764083862305\n",
      "Loss after 35 iterations: 3.2440593242645264\n",
      "Loss after 36 iterations: 3.233060836791992\n",
      "Loss after 37 iterations: 3.2220804691314697\n",
      "Loss after 38 iterations: 3.2111189365386963\n",
      "Loss after 39 iterations: 3.2001760005950928\n",
      "Loss after 40 iterations: 3.189250946044922\n",
      "Loss after 41 iterations: 3.1783456802368164\n",
      "Loss after 42 iterations: 3.1674587726593018\n",
      "Loss after 43 iterations: 3.1565911769866943\n",
      "Loss after 44 iterations: 3.145742654800415\n",
      "Loss after 45 iterations: 3.134913444519043\n",
      "Loss after 46 iterations: 3.124103307723999\n",
      "Loss after 47 iterations: 3.1133127212524414\n",
      "Loss after 48 iterations: 3.102541923522949\n",
      "Loss after 49 iterations: 3.0917904376983643\n",
      "Loss after 50 iterations: 3.081059217453003\n",
      "Loss after 51 iterations: 3.070347785949707\n",
      "Loss after 52 iterations: 3.0596561431884766\n",
      "Loss after 53 iterations: 3.0489847660064697\n",
      "Loss after 54 iterations: 3.0383341312408447\n",
      "Loss after 55 iterations: 3.027703285217285\n",
      "Loss after 56 iterations: 3.0170929431915283\n",
      "Loss after 57 iterations: 3.0065038204193115\n",
      "Loss after 58 iterations: 2.9959347248077393\n",
      "Loss after 59 iterations: 2.985386610031128\n",
      "Loss after 60 iterations: 2.974860191345215\n",
      "Loss after 61 iterations: 2.9643542766571045\n",
      "Loss after 62 iterations: 2.953869581222534\n",
      "Loss after 63 iterations: 2.943406105041504\n",
      "Loss after 64 iterations: 2.932964324951172\n",
      "Loss after 65 iterations: 2.922544479370117\n",
      "Loss after 66 iterations: 2.9121456146240234\n",
      "Loss after 67 iterations: 2.901768922805786\n",
      "Loss after 68 iterations: 2.8914146423339844\n",
      "Loss after 69 iterations: 2.88108229637146\n",
      "Loss after 70 iterations: 2.870771646499634\n",
      "Loss after 71 iterations: 2.8604838848114014\n",
      "Loss after 72 iterations: 2.8502180576324463\n",
      "Loss after 73 iterations: 2.839975357055664\n",
      "Loss after 74 iterations: 2.8297548294067383\n",
      "Loss after 75 iterations: 2.8195571899414062\n",
      "Loss after 76 iterations: 2.8093831539154053\n",
      "Loss after 77 iterations: 2.799231767654419\n",
      "Loss after 78 iterations: 2.7891035079956055\n",
      "Loss after 79 iterations: 2.778998613357544\n",
      "Loss after 80 iterations: 2.7689170837402344\n",
      "Loss after 81 iterations: 2.7588589191436768\n",
      "Loss after 82 iterations: 2.7488248348236084\n",
      "Loss after 83 iterations: 2.73881459236145\n",
      "Loss after 84 iterations: 2.728828191757202\n",
      "Loss after 85 iterations: 2.7188656330108643\n",
      "Loss after 86 iterations: 2.7089273929595947\n",
      "Loss after 87 iterations: 2.6990134716033936\n",
      "Loss after 88 iterations: 2.68912410736084\n",
      "Loss after 89 iterations: 2.6792590618133545\n",
      "Loss after 90 iterations: 2.6694180965423584\n",
      "Loss after 91 iterations: 2.659602403640747\n",
      "Loss after 92 iterations: 2.6498115062713623\n",
      "Loss after 93 iterations: 2.6400458812713623\n",
      "Loss after 94 iterations: 2.6303048133850098\n",
      "Loss after 95 iterations: 2.620589017868042\n",
      "Loss after 96 iterations: 2.610898733139038\n",
      "Loss after 97 iterations: 2.601233959197998\n",
      "Loss after 98 iterations: 2.5915942192077637\n",
      "Loss after 99 iterations: 2.5819802284240723\n",
      "Loss after 100 iterations: 2.572392225265503\n",
      "Loss after 101 iterations: 2.5628297328948975\n",
      "Loss after 102 iterations: 2.553292989730835\n",
      "Loss after 103 iterations: 2.5437824726104736\n",
      "Loss after 104 iterations: 2.5342981815338135\n",
      "Loss after 105 iterations: 2.5248401165008545\n",
      "Loss after 106 iterations: 2.5154080390930176\n",
      "Loss after 107 iterations: 2.50600266456604\n",
      "Loss after 108 iterations: 2.4966235160827637\n",
      "Loss after 109 iterations: 2.487271308898926\n",
      "Loss after 110 iterations: 2.47794508934021\n",
      "Loss after 111 iterations: 2.468646287918091\n",
      "Loss after 112 iterations: 2.459373712539673\n",
      "Loss after 113 iterations: 2.4501285552978516\n",
      "Loss after 114 iterations: 2.4409101009368896\n",
      "Loss after 115 iterations: 2.4317188262939453\n",
      "Loss after 116 iterations: 2.4225547313690186\n",
      "Loss after 117 iterations: 2.4134175777435303\n",
      "Loss after 118 iterations: 2.4043076038360596\n",
      "Loss after 119 iterations: 2.3952255249023438\n",
      "Loss after 120 iterations: 2.3861703872680664\n",
      "Loss after 121 iterations: 2.377143144607544\n",
      "Loss after 122 iterations: 2.368143081665039\n",
      "Loss after 123 iterations: 2.35917067527771\n",
      "Loss after 124 iterations: 2.3502261638641357\n",
      "Loss after 125 iterations: 2.3413093090057373\n",
      "Loss after 126 iterations: 2.3324201107025146\n",
      "Loss after 127 iterations: 2.323559045791626\n",
      "Loss after 128 iterations: 2.314725399017334\n",
      "Loss after 129 iterations: 2.305919885635376\n",
      "Loss after 130 iterations: 2.297142267227173\n",
      "Loss after 131 iterations: 2.288393259048462\n",
      "Loss after 132 iterations: 2.2796716690063477\n",
      "Loss after 133 iterations: 2.2709784507751465\n",
      "Loss after 134 iterations: 2.2623131275177\n",
      "Loss after 135 iterations: 2.253676176071167\n",
      "Loss after 136 iterations: 2.245067834854126\n",
      "Loss after 137 iterations: 2.23648738861084\n",
      "Loss after 138 iterations: 2.227935552597046\n",
      "Loss after 139 iterations: 2.219411611557007\n",
      "Loss after 140 iterations: 2.21091628074646\n",
      "Loss after 141 iterations: 2.202449083328247\n",
      "Loss after 142 iterations: 2.1940102577209473\n",
      "Loss after 143 iterations: 2.185600519180298\n",
      "Loss after 144 iterations: 2.177218198776245\n",
      "Loss after 145 iterations: 2.1688649654388428\n",
      "Loss after 146 iterations: 2.1605401039123535\n",
      "Loss after 147 iterations: 2.1522438526153564\n",
      "Loss after 148 iterations: 2.1439757347106934\n",
      "Loss after 149 iterations: 2.1357364654541016\n",
      "Loss after 150 iterations: 2.127525568008423\n",
      "Loss after 151 iterations: 2.1193430423736572\n",
      "Loss after 152 iterations: 2.111189126968384\n",
      "Loss after 153 iterations: 2.1030638217926025\n",
      "Loss after 154 iterations: 2.0949671268463135\n",
      "Loss after 155 iterations: 2.0868985652923584\n",
      "Loss after 156 iterations: 2.0788588523864746\n",
      "Loss after 157 iterations: 2.070847749710083\n",
      "Loss after 158 iterations: 2.0628645420074463\n",
      "Loss after 159 iterations: 2.05491042137146\n",
      "Loss after 160 iterations: 2.0469844341278076\n",
      "Loss after 161 iterations: 2.0390870571136475\n",
      "Loss after 162 iterations: 2.0312180519104004\n",
      "Loss after 163 iterations: 2.0233776569366455\n",
      "Loss after 164 iterations: 2.0155656337738037\n",
      "Loss after 165 iterations: 2.007781744003296\n",
      "Loss after 166 iterations: 2.0000264644622803\n",
      "Loss after 167 iterations: 1.9922996759414673\n",
      "Loss after 168 iterations: 1.9846009016036987\n",
      "Loss after 169 iterations: 1.9769306182861328\n",
      "Loss after 170 iterations: 1.9692883491516113\n",
      "Loss after 171 iterations: 1.961674690246582\n",
      "Loss after 172 iterations: 1.9540892839431763\n",
      "Loss after 173 iterations: 1.9465317726135254\n",
      "Loss after 174 iterations: 1.9390026330947876\n",
      "Loss after 175 iterations: 1.9315013885498047\n",
      "Loss after 176 iterations: 1.924028754234314\n",
      "Loss after 177 iterations: 1.9165838956832886\n",
      "Loss after 178 iterations: 1.9091671705245972\n",
      "Loss after 179 iterations: 1.901778221130371\n",
      "Loss after 180 iterations: 1.894417405128479\n",
      "Loss after 181 iterations: 1.8870843648910522\n",
      "Loss after 182 iterations: 1.87977933883667\n",
      "Loss after 183 iterations: 1.8725019693374634\n",
      "Loss after 184 iterations: 1.8652526140213013\n",
      "Loss after 185 iterations: 1.858030915260315\n",
      "Loss after 186 iterations: 1.850837230682373\n",
      "Loss after 187 iterations: 1.8436707258224487\n",
      "Loss after 188 iterations: 1.8365319967269897\n",
      "Loss after 189 iterations: 1.829421043395996\n",
      "Loss after 190 iterations: 1.8223376274108887\n",
      "Loss after 191 iterations: 1.8152815103530884\n",
      "Loss after 192 iterations: 1.8082528114318848\n",
      "Loss after 193 iterations: 1.8012514114379883\n",
      "Loss after 194 iterations: 1.7942776679992676\n",
      "Loss after 195 iterations: 1.7873311042785645\n",
      "Loss after 196 iterations: 1.780411720275879\n",
      "Loss after 197 iterations: 1.7735196352005005\n",
      "Loss after 198 iterations: 1.7666544914245605\n",
      "Loss after 199 iterations: 1.7598166465759277\n",
      "Loss after 200 iterations: 1.753005862236023\n",
      "Loss after 201 iterations: 1.746221899986267\n",
      "Loss after 202 iterations: 1.7394648790359497\n",
      "Loss after 203 iterations: 1.7327346801757812\n",
      "Loss after 204 iterations: 1.7260313034057617\n",
      "Loss after 205 iterations: 1.7193547487258911\n",
      "Loss after 206 iterations: 1.7127050161361694\n",
      "Loss after 207 iterations: 1.706081509590149\n",
      "Loss after 208 iterations: 1.699485182762146\n",
      "Loss after 209 iterations: 1.6929149627685547\n",
      "Loss after 210 iterations: 1.6863713264465332\n",
      "Loss after 211 iterations: 1.6798540353775024\n",
      "Loss after 212 iterations: 1.6733633279800415\n",
      "Loss after 213 iterations: 1.6668988466262817\n",
      "Loss after 214 iterations: 1.6604608297348022\n",
      "Loss after 215 iterations: 1.6540485620498657\n",
      "Loss after 216 iterations: 1.6476627588272095\n",
      "Loss after 217 iterations: 1.6413029432296753\n",
      "Loss after 218 iterations: 1.6349691152572632\n",
      "Loss after 219 iterations: 1.6286611557006836\n",
      "Loss after 220 iterations: 1.6223793029785156\n",
      "Loss after 221 iterations: 1.616123080253601\n",
      "Loss after 222 iterations: 1.609892725944519\n",
      "Loss after 223 iterations: 1.60368812084198\n",
      "Loss after 224 iterations: 1.5975090265274048\n",
      "Loss after 225 iterations: 1.5913556814193726\n",
      "Loss after 226 iterations: 1.5852278470993042\n",
      "Loss after 227 iterations: 1.5791254043579102\n",
      "Loss after 228 iterations: 1.5730482339859009\n",
      "Loss after 229 iterations: 1.566996693611145\n",
      "Loss after 230 iterations: 1.560970425605774\n",
      "Loss after 231 iterations: 1.5549694299697876\n",
      "Loss after 232 iterations: 1.5489932298660278\n",
      "Loss after 233 iterations: 1.5430426597595215\n",
      "Loss after 234 iterations: 1.5371168851852417\n",
      "Loss after 235 iterations: 1.5312161445617676\n",
      "Loss after 236 iterations: 1.52534019947052\n",
      "Loss after 237 iterations: 1.5194891691207886\n",
      "Loss after 238 iterations: 1.5136629343032837\n",
      "Loss after 239 iterations: 1.507861614227295\n",
      "Loss after 240 iterations: 1.502084732055664\n",
      "Loss after 241 iterations: 1.4963326454162598\n",
      "Loss after 242 iterations: 1.4906049966812134\n",
      "Loss after 243 iterations: 1.484902024269104\n",
      "Loss after 244 iterations: 1.4792232513427734\n",
      "Loss after 245 iterations: 1.4735690355300903\n",
      "Loss after 246 iterations: 1.4679388999938965\n",
      "Loss after 247 iterations: 1.4623332023620605\n",
      "Loss after 248 iterations: 1.4567519426345825\n",
      "Loss after 249 iterations: 1.451194167137146\n",
      "Loss after 250 iterations: 1.445660948753357\n",
      "Loss after 251 iterations: 1.4401516914367676\n",
      "Loss after 252 iterations: 1.4346661567687988\n",
      "Loss after 253 iterations: 1.4292045831680298\n",
      "Loss after 254 iterations: 1.4237669706344604\n",
      "Loss after 255 iterations: 1.418352723121643\n",
      "Loss after 256 iterations: 1.412962555885315\n",
      "Loss after 257 iterations: 1.4075957536697388\n",
      "Loss after 258 iterations: 1.4022526741027832\n",
      "Loss after 259 iterations: 1.3969330787658691\n",
      "Loss after 260 iterations: 1.3916369676589966\n",
      "Loss after 261 iterations: 1.3863641023635864\n",
      "Loss after 262 iterations: 1.3811148405075073\n",
      "Loss after 263 iterations: 1.375888466835022\n",
      "Loss after 264 iterations: 1.3706854581832886\n",
      "Loss after 265 iterations: 1.365505576133728\n",
      "Loss after 266 iterations: 1.3603487014770508\n",
      "Loss after 267 iterations: 1.3552147150039673\n",
      "Loss after 268 iterations: 1.3501038551330566\n",
      "Loss after 269 iterations: 1.3450156450271606\n",
      "Loss after 270 iterations: 1.3399505615234375\n",
      "Loss after 271 iterations: 1.3349080085754395\n",
      "Loss after 272 iterations: 1.3298882246017456\n",
      "Loss after 273 iterations: 1.3248909711837769\n",
      "Loss after 274 iterations: 1.3199163675308228\n",
      "Loss after 275 iterations: 1.3149641752243042\n",
      "Loss after 276 iterations: 1.3100346326828003\n",
      "Loss after 277 iterations: 1.3051271438598633\n",
      "Loss after 278 iterations: 1.3002420663833618\n",
      "Loss after 279 iterations: 1.2953792810440063\n",
      "Loss after 280 iterations: 1.2905385494232178\n",
      "Loss after 281 iterations: 1.2857201099395752\n",
      "Loss after 282 iterations: 1.2809237241744995\n",
      "Loss after 283 iterations: 1.2761491537094116\n",
      "Loss after 284 iterations: 1.2713967561721802\n",
      "Loss after 285 iterations: 1.266666054725647\n",
      "Loss after 286 iterations: 1.261957049369812\n",
      "Loss after 287 iterations: 1.2572699785232544\n",
      "Loss after 288 iterations: 1.252604365348816\n",
      "Loss after 289 iterations: 1.2479605674743652\n",
      "Loss after 290 iterations: 1.2433382272720337\n",
      "Loss after 291 iterations: 1.2387372255325317\n",
      "Loss after 292 iterations: 1.2341578006744385\n",
      "Loss after 293 iterations: 1.2295995950698853\n",
      "Loss after 294 iterations: 1.2250627279281616\n",
      "Loss after 295 iterations: 1.2205471992492676\n",
      "Loss after 296 iterations: 1.2160526514053345\n",
      "Loss after 297 iterations: 1.2115792036056519\n",
      "Loss after 298 iterations: 1.2071267366409302\n",
      "Loss after 299 iterations: 1.2026952505111694\n",
      "Loss after 300 iterations: 1.1982848644256592\n",
      "Loss after 301 iterations: 1.1938949823379517\n",
      "Loss after 302 iterations: 1.1895259618759155\n",
      "Loss after 303 iterations: 1.1851778030395508\n",
      "Loss after 304 iterations: 1.1808499097824097\n",
      "Loss after 305 iterations: 1.17654287815094\n",
      "Loss after 306 iterations: 1.172256350517273\n",
      "Loss after 307 iterations: 1.16798996925354\n",
      "Loss after 308 iterations: 1.1637442111968994\n",
      "Loss after 309 iterations: 1.1595185995101929\n",
      "Loss after 310 iterations: 1.15531325340271\n",
      "Loss after 311 iterations: 1.1511281728744507\n",
      "Loss after 312 iterations: 1.1469630002975464\n",
      "Loss after 313 iterations: 1.1428178548812866\n",
      "Loss after 314 iterations: 1.138692855834961\n",
      "Loss after 315 iterations: 1.1345875263214111\n",
      "Loss after 316 iterations: 1.1305021047592163\n",
      "Loss after 317 iterations: 1.126436471939087\n",
      "Loss after 318 iterations: 1.122390627861023\n",
      "Loss after 319 iterations: 1.1183642148971558\n",
      "Loss after 320 iterations: 1.114357590675354\n",
      "Loss after 321 iterations: 1.11037015914917\n",
      "Loss after 322 iterations: 1.1064023971557617\n",
      "Loss after 323 iterations: 1.1024538278579712\n",
      "Loss after 324 iterations: 1.098524570465088\n",
      "Loss after 325 iterations: 1.0946146249771118\n",
      "Loss after 326 iterations: 1.0907237529754639\n",
      "Loss after 327 iterations: 1.086851954460144\n",
      "Loss after 328 iterations: 1.082999348640442\n",
      "Loss after 329 iterations: 1.0791651010513306\n",
      "Loss after 330 iterations: 1.0753504037857056\n",
      "Loss after 331 iterations: 1.0715540647506714\n",
      "Loss after 332 iterations: 1.0677767992019653\n",
      "Loss after 333 iterations: 1.06401789188385\n",
      "Loss after 334 iterations: 1.0602777004241943\n",
      "Loss after 335 iterations: 1.0565561056137085\n",
      "Loss after 336 iterations: 1.052852988243103\n",
      "Loss after 337 iterations: 1.0491681098937988\n",
      "Loss after 338 iterations: 1.0455015897750854\n",
      "Loss after 339 iterations: 1.041853427886963\n",
      "Loss after 340 iterations: 1.038223385810852\n",
      "Loss after 341 iterations: 1.0346113443374634\n",
      "Loss after 342 iterations: 1.031017541885376\n",
      "Loss after 343 iterations: 1.0274416208267212\n",
      "Loss after 344 iterations: 1.023883581161499\n",
      "Loss after 345 iterations: 1.0203434228897095\n",
      "Loss after 346 iterations: 1.016821026802063\n",
      "Loss after 347 iterations: 1.0133161544799805\n",
      "Loss after 348 iterations: 1.0098291635513306\n",
      "Loss after 349 iterations: 1.006359577178955\n",
      "Loss after 350 iterations: 1.002907395362854\n",
      "Loss after 351 iterations: 0.9994726777076721\n",
      "Loss after 352 iterations: 0.9960554242134094\n",
      "Loss after 353 iterations: 0.9926554560661316\n",
      "Loss after 354 iterations: 0.9892725944519043\n",
      "Loss after 355 iterations: 0.9859066605567932\n",
      "Loss after 356 iterations: 0.9825580716133118\n",
      "Loss after 357 iterations: 0.9792263507843018\n",
      "Loss after 358 iterations: 0.9759116172790527\n",
      "Loss after 359 iterations: 0.9726138114929199\n",
      "Loss after 360 iterations: 0.9693326354026794\n",
      "Loss after 361 iterations: 0.9660682082176208\n",
      "Loss after 362 iterations: 0.962820291519165\n",
      "Loss after 363 iterations: 0.9595891833305359\n",
      "Loss after 364 iterations: 0.9563744068145752\n",
      "Loss after 365 iterations: 0.9531760811805725\n",
      "Loss after 366 iterations: 0.9499940872192383\n",
      "Loss after 367 iterations: 0.9468285441398621\n",
      "Loss after 368 iterations: 0.9436790347099304\n",
      "Loss after 369 iterations: 0.9405456185340881\n",
      "Loss after 370 iterations: 0.9374284744262695\n",
      "Loss after 371 iterations: 0.9343271851539612\n",
      "Loss after 372 iterations: 0.9312419295310974\n",
      "Loss after 373 iterations: 0.9281724095344543\n",
      "Loss after 374 iterations: 0.9251187443733215\n",
      "Loss after 375 iterations: 0.9220808148384094\n",
      "Loss after 376 iterations: 0.9190585017204285\n",
      "Loss after 377 iterations: 0.9160518050193787\n",
      "Loss after 378 iterations: 0.9130606651306152\n",
      "Loss after 379 iterations: 0.9100847840309143\n",
      "Loss after 380 iterations: 0.9071243405342102\n",
      "Loss after 381 iterations: 0.9041793346405029\n",
      "Loss after 382 iterations: 0.9012494087219238\n",
      "Loss after 383 iterations: 0.8983346819877625\n",
      "Loss after 384 iterations: 0.895435094833374\n",
      "Loss after 385 iterations: 0.8925504684448242\n",
      "Loss after 386 iterations: 0.8896808624267578\n",
      "Loss after 387 iterations: 0.8868259787559509\n",
      "Loss after 388 iterations: 0.8839859962463379\n",
      "Loss after 389 iterations: 0.8811607360839844\n",
      "Loss after 390 iterations: 0.8783500790596008\n",
      "Loss after 391 iterations: 0.8755542635917664\n",
      "Loss after 392 iterations: 0.872772753238678\n",
      "Loss after 393 iterations: 0.8700056672096252\n",
      "Loss after 394 iterations: 0.8672530651092529\n",
      "Loss after 395 iterations: 0.8645148277282715\n",
      "Loss after 396 iterations: 0.8617907166481018\n",
      "Loss after 397 iterations: 0.8590808510780334\n",
      "Loss after 398 iterations: 0.8563850522041321\n",
      "Loss after 399 iterations: 0.8537033200263977\n",
      "Loss after 400 iterations: 0.8510355949401855\n",
      "Loss after 401 iterations: 0.8483818173408508\n",
      "Loss after 402 iterations: 0.8457417488098145\n",
      "Loss after 403 iterations: 0.8431156277656555\n",
      "Loss after 404 iterations: 0.8405030369758606\n",
      "Loss after 405 iterations: 0.8379040360450745\n",
      "Loss after 406 iterations: 0.8353187441825867\n",
      "Loss after 407 iterations: 0.8327469229698181\n",
      "Loss after 408 iterations: 0.8301884531974792\n",
      "Loss after 409 iterations: 0.8276433348655701\n",
      "Loss after 410 iterations: 0.8251115679740906\n",
      "Loss after 411 iterations: 0.8225930333137512\n",
      "Loss after 412 iterations: 0.8200876712799072\n",
      "Loss after 413 iterations: 0.8175953030586243\n",
      "Loss after 414 iterations: 0.8151161074638367\n",
      "Loss after 415 iterations: 0.8126497864723206\n",
      "Loss after 416 iterations: 0.8101963996887207\n",
      "Loss after 417 iterations: 0.8077558875083923\n",
      "Loss after 418 iterations: 0.8053278923034668\n",
      "Loss after 419 iterations: 0.802912712097168\n",
      "Loss after 420 iterations: 0.8005102276802063\n",
      "Loss after 421 iterations: 0.7981203198432922\n",
      "Loss after 422 iterations: 0.7957427501678467\n",
      "Loss after 423 iterations: 0.793377697467804\n",
      "Loss after 424 iterations: 0.7910249829292297\n",
      "Loss after 425 iterations: 0.7886847853660583\n",
      "Loss after 426 iterations: 0.786356508731842\n",
      "Loss after 427 iterations: 0.7840404510498047\n",
      "Loss after 428 iterations: 0.7817366123199463\n",
      "Loss after 429 iterations: 0.779444694519043\n",
      "Loss after 430 iterations: 0.7771647572517395\n",
      "Loss after 431 iterations: 0.7748968601226807\n",
      "Loss after 432 iterations: 0.7726407051086426\n",
      "Loss after 433 iterations: 0.7703964114189148\n",
      "Loss after 434 iterations: 0.7681637406349182\n",
      "Loss after 435 iterations: 0.7659427523612976\n",
      "Loss after 436 iterations: 0.7637333869934082\n",
      "Loss after 437 iterations: 0.7615354061126709\n",
      "Loss after 438 iterations: 0.7593491673469543\n",
      "Loss after 439 iterations: 0.7571741938591003\n",
      "Loss after 440 iterations: 0.7550104260444641\n",
      "Loss after 441 iterations: 0.7528581023216248\n",
      "Loss after 442 iterations: 0.7507168650627136\n",
      "Loss after 443 iterations: 0.748586893081665\n",
      "Loss after 444 iterations: 0.7464680075645447\n",
      "Loss after 445 iterations: 0.7443599700927734\n",
      "Loss after 446 iterations: 0.74226313829422\n",
      "Loss after 447 iterations: 0.7401770949363708\n",
      "Loss after 448 iterations: 0.7381019592285156\n",
      "Loss after 449 iterations: 0.7360376715660095\n",
      "Loss after 450 iterations: 0.7339839935302734\n",
      "Loss after 451 iterations: 0.7319410443305969\n",
      "Loss after 452 iterations: 0.7299086451530457\n",
      "Loss after 453 iterations: 0.727886974811554\n",
      "Loss after 454 iterations: 0.7258755564689636\n",
      "Loss after 455 iterations: 0.7238747477531433\n",
      "Loss after 456 iterations: 0.7218841910362244\n",
      "Loss after 457 iterations: 0.7199040055274963\n",
      "Loss after 458 iterations: 0.7179340720176697\n",
      "Loss after 459 iterations: 0.7159743905067444\n",
      "Loss after 460 iterations: 0.7140247225761414\n",
      "Loss after 461 iterations: 0.7120852470397949\n",
      "Loss after 462 iterations: 0.7101556658744812\n",
      "Loss after 463 iterations: 0.7082362174987793\n",
      "Loss after 464 iterations: 0.7063265442848206\n",
      "Loss after 465 iterations: 0.7044268250465393\n",
      "Loss after 466 iterations: 0.7025368213653564\n",
      "Loss after 467 iterations: 0.7006566524505615\n",
      "Loss after 468 iterations: 0.698786199092865\n",
      "Loss after 469 iterations: 0.6969252228736877\n",
      "Loss after 470 iterations: 0.6950740218162537\n",
      "Loss after 471 iterations: 0.6932321190834045\n",
      "Loss after 472 iterations: 0.6913998126983643\n",
      "Loss after 473 iterations: 0.6895768642425537\n",
      "Loss after 474 iterations: 0.6877632737159729\n",
      "Loss after 475 iterations: 0.6859590411186218\n",
      "Loss after 476 iterations: 0.6841639876365662\n",
      "Loss after 477 iterations: 0.6823781132698059\n",
      "Loss after 478 iterations: 0.6806014180183411\n",
      "Loss after 479 iterations: 0.6788339018821716\n",
      "Loss after 480 iterations: 0.6770753860473633\n",
      "Loss after 481 iterations: 0.6753258109092712\n",
      "Loss after 482 iterations: 0.6735851168632507\n",
      "Loss after 483 iterations: 0.671853244304657\n",
      "Loss after 484 iterations: 0.6701303124427795\n",
      "Loss after 485 iterations: 0.6684160828590393\n",
      "Loss after 486 iterations: 0.666710615158081\n",
      "Loss after 487 iterations: 0.6650139093399048\n",
      "Loss after 488 iterations: 0.6633257269859314\n",
      "Loss after 489 iterations: 0.6616460680961609\n",
      "Loss after 490 iterations: 0.659974992275238\n",
      "Loss after 491 iterations: 0.6583123803138733\n",
      "Loss after 492 iterations: 0.6566581726074219\n",
      "Loss after 493 iterations: 0.6550124287605286\n",
      "Loss after 494 iterations: 0.6533748507499695\n",
      "Loss after 495 iterations: 0.651745617389679\n",
      "Loss after 496 iterations: 0.6501246094703674\n",
      "Loss after 497 iterations: 0.6485118269920349\n",
      "Loss after 498 iterations: 0.6469070315361023\n",
      "Loss after 499 iterations: 0.6453103423118591\n",
      "Loss after 500 iterations: 0.6437218189239502\n",
      "Loss after 501 iterations: 0.6421411633491516\n",
      "Loss after 502 iterations: 0.6405684947967529\n",
      "Loss after 503 iterations: 0.6390036940574646\n",
      "Loss after 504 iterations: 0.6374465823173523\n",
      "Loss after 505 iterations: 0.6358974575996399\n",
      "Loss after 506 iterations: 0.6343559622764587\n",
      "Loss after 507 iterations: 0.6328222155570984\n",
      "Loss after 508 iterations: 0.6312962174415588\n",
      "Loss after 509 iterations: 0.629777729511261\n",
      "Loss after 510 iterations: 0.6282668113708496\n",
      "Loss after 511 iterations: 0.6267634034156799\n",
      "Loss after 512 iterations: 0.6252674460411072\n",
      "Loss after 513 iterations: 0.6237790584564209\n",
      "Loss after 514 iterations: 0.6222979426383972\n",
      "Loss after 515 iterations: 0.6208241581916809\n",
      "Loss after 516 iterations: 0.6193576455116272\n",
      "Loss after 517 iterations: 0.6178984642028809\n",
      "Loss after 518 iterations: 0.6164464950561523\n",
      "Loss after 519 iterations: 0.6150016188621521\n",
      "Loss after 520 iterations: 0.6135638952255249\n",
      "Loss after 521 iterations: 0.6121332049369812\n",
      "Loss after 522 iterations: 0.6107096076011658\n",
      "Loss after 523 iterations: 0.6092930436134338\n",
      "Loss after 524 iterations: 0.6078833341598511\n",
      "Loss after 525 iterations: 0.6064805388450623\n",
      "Loss after 526 iterations: 0.6050847172737122\n",
      "Loss after 527 iterations: 0.6036956906318665\n",
      "Loss after 528 iterations: 0.6023134589195251\n",
      "Loss after 529 iterations: 0.6009378433227539\n",
      "Loss after 530 iterations: 0.5995691418647766\n",
      "Loss after 531 iterations: 0.5982069373130798\n",
      "Loss after 532 iterations: 0.5968514680862427\n",
      "Loss after 533 iterations: 0.5955025553703308\n",
      "Loss after 534 iterations: 0.5941601991653442\n",
      "Loss after 535 iterations: 0.5928242802619934\n",
      "Loss after 536 iterations: 0.5914949178695679\n",
      "Loss after 537 iterations: 0.5901719331741333\n",
      "Loss after 538 iterations: 0.5888553857803345\n",
      "Loss after 539 iterations: 0.5875450968742371\n",
      "Loss after 540 iterations: 0.5862411856651306\n",
      "Loss after 541 iterations: 0.5849434733390808\n",
      "Loss after 542 iterations: 0.5836521983146667\n",
      "Loss after 543 iterations: 0.5823670029640198\n",
      "Loss after 544 iterations: 0.5810878872871399\n",
      "Loss after 545 iterations: 0.5798150300979614\n",
      "Loss after 546 iterations: 0.5785481929779053\n",
      "Loss after 547 iterations: 0.577287495136261\n",
      "Loss after 548 iterations: 0.5760326981544495\n",
      "Loss after 549 iterations: 0.574783980846405\n",
      "Loss after 550 iterations: 0.5735410451889038\n",
      "Loss after 551 iterations: 0.5723041892051697\n",
      "Loss after 552 iterations: 0.5710732340812683\n",
      "Loss after 553 iterations: 0.5698478817939758\n",
      "Loss after 554 iterations: 0.5686285495758057\n",
      "Loss after 555 iterations: 0.5674149394035339\n",
      "Loss after 556 iterations: 0.5662069916725159\n",
      "Loss after 557 iterations: 0.5650048851966858\n",
      "Loss after 558 iterations: 0.5638083219528198\n",
      "Loss after 559 iterations: 0.5626174211502075\n",
      "Loss after 560 iterations: 0.5614320635795593\n",
      "Loss after 561 iterations: 0.56025230884552\n",
      "Loss after 562 iterations: 0.5590781569480896\n",
      "Loss after 563 iterations: 0.5579094290733337\n",
      "Loss after 564 iterations: 0.5567461252212524\n",
      "Loss after 565 iterations: 0.5555883646011353\n",
      "Loss after 566 iterations: 0.5544360280036926\n",
      "Loss after 567 iterations: 0.5532888770103455\n",
      "Loss after 568 iterations: 0.5521471500396729\n",
      "Loss after 569 iterations: 0.55101078748703\n",
      "Loss after 570 iterations: 0.5498796701431274\n",
      "Loss after 571 iterations: 0.5487537384033203\n",
      "Loss after 572 iterations: 0.5476330518722534\n",
      "Loss after 573 iterations: 0.546517550945282\n",
      "Loss after 574 iterations: 0.5454071164131165\n",
      "Loss after 575 iterations: 0.5443019270896912\n",
      "Loss after 576 iterations: 0.5432016849517822\n",
      "Loss after 577 iterations: 0.5421066284179688\n",
      "Loss after 578 iterations: 0.5410165190696716\n",
      "Loss after 579 iterations: 0.5399314761161804\n",
      "Loss after 580 iterations: 0.5388513207435608\n",
      "Loss after 581 iterations: 0.5377760529518127\n",
      "Loss after 582 iterations: 0.5367057919502258\n",
      "Loss after 583 iterations: 0.5356403589248657\n",
      "Loss after 584 iterations: 0.5345798134803772\n",
      "Loss after 585 iterations: 0.5335240960121155\n",
      "Loss after 586 iterations: 0.5324731469154358\n",
      "Loss after 587 iterations: 0.5314269065856934\n",
      "Loss after 588 iterations: 0.530385434627533\n",
      "Loss after 589 iterations: 0.5293486714363098\n",
      "Loss after 590 iterations: 0.5283166170120239\n",
      "Loss after 591 iterations: 0.527289092540741\n",
      "Loss after 592 iterations: 0.5262663960456848\n",
      "Loss after 593 iterations: 0.525248110294342\n",
      "Loss after 594 iterations: 0.5242345929145813\n",
      "Loss after 595 iterations: 0.5232253670692444\n",
      "Loss after 596 iterations: 0.5222208499908447\n",
      "Loss after 597 iterations: 0.5212206840515137\n",
      "Loss after 598 iterations: 0.5202250480651855\n",
      "Loss after 599 iterations: 0.5192339420318604\n",
      "Loss after 600 iterations: 0.5182470679283142\n",
      "Loss after 601 iterations: 0.5172646641731262\n",
      "Loss after 602 iterations: 0.5162866711616516\n",
      "Loss after 603 iterations: 0.5153128504753113\n",
      "Loss after 604 iterations: 0.5143434405326843\n",
      "Loss after 605 iterations: 0.5133783221244812\n",
      "Loss after 606 iterations: 0.5124174356460571\n",
      "Loss after 607 iterations: 0.5114607214927673\n",
      "Loss after 608 iterations: 0.5105082392692566\n",
      "Loss after 609 iterations: 0.5095601081848145\n",
      "Loss after 610 iterations: 0.5086159110069275\n",
      "Loss after 611 iterations: 0.5076759457588196\n",
      "Loss after 612 iterations: 0.5067400336265564\n",
      "Loss after 613 iterations: 0.5058082938194275\n",
      "Loss after 614 iterations: 0.5048804879188538\n",
      "Loss after 615 iterations: 0.5039569139480591\n",
      "Loss after 616 iterations: 0.5030372738838196\n",
      "Loss after 617 iterations: 0.5021215081214905\n",
      "Loss after 618 iterations: 0.5012097954750061\n",
      "Loss after 619 iterations: 0.5003020167350769\n",
      "Loss after 620 iterations: 0.4993981122970581\n",
      "Loss after 621 iterations: 0.49849817156791687\n",
      "Loss after 622 iterations: 0.49760207533836365\n",
      "Loss after 623 iterations: 0.4967099130153656\n",
      "Loss after 624 iterations: 0.4958215057849884\n",
      "Loss after 625 iterations: 0.4949369430541992\n",
      "Loss after 626 iterations: 0.4940561354160309\n",
      "Loss after 627 iterations: 0.4931791126728058\n",
      "Loss after 628 iterations: 0.492305725812912\n",
      "Loss after 629 iterations: 0.491436243057251\n",
      "Loss after 630 iterations: 0.4905703365802765\n",
      "Loss after 631 iterations: 0.48970815539360046\n",
      "Loss after 632 iterations: 0.48884961009025574\n",
      "Loss after 633 iterations: 0.4879947006702423\n",
      "Loss after 634 iterations: 0.48714348673820496\n",
      "Loss after 635 iterations: 0.4862957298755646\n",
      "Loss after 636 iterations: 0.48545166850090027\n",
      "Loss after 637 iterations: 0.48461106419563293\n",
      "Loss after 638 iterations: 0.4837740659713745\n",
      "Loss after 639 iterations: 0.48294055461883545\n",
      "Loss after 640 iterations: 0.48211050033569336\n",
      "Loss after 641 iterations: 0.4812839925289154\n",
      "Loss after 642 iterations: 0.48046091198921204\n",
      "Loss after 643 iterations: 0.479641318321228\n",
      "Loss after 644 iterations: 0.47882506251335144\n",
      "Loss after 645 iterations: 0.47801217436790466\n",
      "Loss after 646 iterations: 0.47720274329185486\n",
      "Loss after 647 iterations: 0.4763965606689453\n",
      "Loss after 648 iterations: 0.4755938947200775\n",
      "Loss after 649 iterations: 0.4747943580150604\n",
      "Loss after 650 iterations: 0.4739982783794403\n",
      "Loss after 651 iterations: 0.4732053279876709\n",
      "Loss after 652 iterations: 0.4724157750606537\n",
      "Loss after 653 iterations: 0.47162938117980957\n",
      "Loss after 654 iterations: 0.4708462655544281\n",
      "Loss after 655 iterations: 0.4700663387775421\n",
      "Loss after 656 iterations: 0.4692895710468292\n",
      "Loss after 657 iterations: 0.4685159921646118\n",
      "Loss after 658 iterations: 0.4677456319332123\n",
      "Loss after 659 iterations: 0.4669783115386963\n",
      "Loss after 660 iterations: 0.46621420979499817\n",
      "Loss after 661 iterations: 0.4654531180858612\n",
      "Loss after 662 iterations: 0.4646952152252197\n",
      "Loss after 663 iterations: 0.46394026279449463\n",
      "Loss after 664 iterations: 0.46318840980529785\n",
      "Loss after 665 iterations: 0.462439626455307\n",
      "Loss after 666 iterations: 0.46169379353523254\n",
      "Loss after 667 iterations: 0.4609510004520416\n",
      "Loss after 668 iterations: 0.4602111577987671\n",
      "Loss after 669 iterations: 0.4594744145870209\n",
      "Loss after 670 iterations: 0.4587404429912567\n",
      "Loss after 671 iterations: 0.4580094516277313\n",
      "Loss after 672 iterations: 0.4572813808917999\n",
      "Loss after 673 iterations: 0.45655620098114014\n",
      "Loss after 674 iterations: 0.4558340311050415\n",
      "Loss after 675 iterations: 0.45511460304260254\n",
      "Loss after 676 iterations: 0.4543980658054352\n",
      "Loss after 677 iterations: 0.45368435978889465\n",
      "Loss after 678 iterations: 0.4529735743999481\n",
      "Loss after 679 iterations: 0.4522654116153717\n",
      "Loss after 680 iterations: 0.4515601396560669\n",
      "Loss after 681 iterations: 0.45085766911506653\n",
      "Loss after 682 iterations: 0.4501579701900482\n",
      "Loss after 683 iterations: 0.4494608938694\n",
      "Loss after 684 iterations: 0.44876670837402344\n",
      "Loss after 685 iterations: 0.4480750858783722\n",
      "Loss after 686 iterations: 0.4473862648010254\n",
      "Loss after 687 iterations: 0.4467000961303711\n",
      "Loss after 688 iterations: 0.4460166394710541\n",
      "Loss after 689 iterations: 0.4453357458114624\n",
      "Loss after 690 iterations: 0.444657564163208\n",
      "Loss after 691 iterations: 0.4439821243286133\n",
      "Loss after 692 iterations: 0.44330906867980957\n",
      "Loss after 693 iterations: 0.4426387548446655\n",
      "Loss after 694 iterations: 0.4419710636138916\n",
      "Loss after 695 iterations: 0.44130587577819824\n",
      "Loss after 696 iterations: 0.44064322113990784\n",
      "Loss after 697 iterations: 0.43998315930366516\n",
      "Loss after 698 iterations: 0.43932560086250305\n",
      "Loss after 699 iterations: 0.4386705160140991\n",
      "Loss after 700 iterations: 0.4380180537700653\n",
      "Loss after 701 iterations: 0.4373679459095001\n",
      "Loss after 702 iterations: 0.43672052025794983\n",
      "Loss after 703 iterations: 0.4360754191875458\n",
      "Loss after 704 iterations: 0.4354327619075775\n",
      "Loss after 705 iterations: 0.43479254841804504\n",
      "Loss after 706 iterations: 0.434154748916626\n",
      "Loss after 707 iterations: 0.4335193932056427\n",
      "Loss after 708 iterations: 0.4328864514827728\n",
      "Loss after 709 iterations: 0.4322558641433716\n",
      "Loss after 710 iterations: 0.43162772059440613\n",
      "Loss after 711 iterations: 0.4310018718242645\n",
      "Loss after 712 iterations: 0.4303783178329468\n",
      "Loss after 713 iterations: 0.42975714802742004\n",
      "Loss after 714 iterations: 0.4291383922100067\n",
      "Loss after 715 iterations: 0.42852187156677246\n",
      "Loss after 716 iterations: 0.42790767550468445\n",
      "Loss after 717 iterations: 0.4272957146167755\n",
      "Loss after 718 iterations: 0.42668601870536804\n",
      "Loss after 719 iterations: 0.4260786771774292\n",
      "Loss after 720 iterations: 0.42547354102134705\n",
      "Loss after 721 iterations: 0.42487064003944397\n",
      "Loss after 722 iterations: 0.42427000403404236\n",
      "Loss after 723 iterations: 0.4236714839935303\n",
      "Loss after 724 iterations: 0.4230753183364868\n",
      "Loss after 725 iterations: 0.4224812090396881\n",
      "Loss after 726 iterations: 0.4218893349170685\n",
      "Loss after 727 iterations: 0.42129960656166077\n",
      "Loss after 728 iterations: 0.42071208357810974\n",
      "Loss after 729 iterations: 0.42012670636177063\n",
      "Loss after 730 iterations: 0.4195435047149658\n",
      "Loss after 731 iterations: 0.4189623296260834\n",
      "Loss after 732 iterations: 0.41838333010673523\n",
      "Loss after 733 iterations: 0.417806476354599\n",
      "Loss after 734 iterations: 0.41723164916038513\n",
      "Loss after 735 iterations: 0.4166589677333832\n",
      "Loss after 736 iterations: 0.4160883128643036\n",
      "Loss after 737 iterations: 0.41551968455314636\n",
      "Loss after 738 iterations: 0.4149531126022339\n",
      "Loss after 739 iterations: 0.41438862681388855\n",
      "Loss after 740 iterations: 0.41382619738578796\n",
      "Loss after 741 iterations: 0.41326573491096497\n",
      "Loss after 742 iterations: 0.41270723938941956\n",
      "Loss after 743 iterations: 0.41215085983276367\n",
      "Loss after 744 iterations: 0.4115964472293854\n",
      "Loss after 745 iterations: 0.41104400157928467\n",
      "Loss after 746 iterations: 0.41049349308013916\n",
      "Loss after 747 iterations: 0.40994492173194885\n",
      "Loss after 748 iterations: 0.40939828753471375\n",
      "Loss after 749 iterations: 0.4088536500930786\n",
      "Loss after 750 iterations: 0.4083109200000763\n",
      "Loss after 751 iterations: 0.4077701270580292\n",
      "Loss after 752 iterations: 0.40723124146461487\n",
      "Loss after 753 iterations: 0.40669429302215576\n",
      "Loss after 754 iterations: 0.40615928173065186\n",
      "Loss after 755 iterations: 0.40562596917152405\n",
      "Loss after 756 iterations: 0.405094712972641\n",
      "Loss after 757 iterations: 0.4045651853084564\n",
      "Loss after 758 iterations: 0.40403756499290466\n",
      "Loss after 759 iterations: 0.40351179242134094\n",
      "Loss after 760 iterations: 0.40298786759376526\n",
      "Loss after 761 iterations: 0.40246573090553284\n",
      "Loss after 762 iterations: 0.4019453823566437\n",
      "Loss after 763 iterations: 0.4014270305633545\n",
      "Loss after 764 iterations: 0.40091022849082947\n",
      "Loss after 765 iterations: 0.40039536356925964\n",
      "Loss after 766 iterations: 0.3998822271823883\n",
      "Loss after 767 iterations: 0.39937087893486023\n",
      "Loss after 768 iterations: 0.398861289024353\n",
      "Loss after 769 iterations: 0.3983534872531891\n",
      "Loss after 770 iterations: 0.39784741401672363\n",
      "Loss after 771 iterations: 0.3973430395126343\n",
      "Loss after 772 iterations: 0.3968404531478882\n",
      "Loss after 773 iterations: 0.3963395357131958\n",
      "Loss after 774 iterations: 0.3958403766155243\n",
      "Loss after 775 iterations: 0.3953428566455841\n",
      "Loss after 776 iterations: 0.3948470652103424\n",
      "Loss after 777 iterations: 0.3943530023097992\n",
      "Loss after 778 iterations: 0.3938605487346649\n",
      "Loss after 779 iterations: 0.39336979389190674\n",
      "Loss after 780 iterations: 0.39288079738616943\n",
      "Loss after 781 iterations: 0.3923933207988739\n",
      "Loss after 782 iterations: 0.3919074833393097\n",
      "Loss after 783 iterations: 0.39142337441444397\n",
      "Loss after 784 iterations: 0.3909408748149872\n",
      "Loss after 785 iterations: 0.39045998454093933\n",
      "Loss after 786 iterations: 0.3899807631969452\n",
      "Loss after 787 iterations: 0.3895030915737152\n",
      "Loss after 788 iterations: 0.38902702927589417\n",
      "Loss after 789 iterations: 0.38855257630348206\n",
      "Loss after 790 iterations: 0.3880796730518341\n",
      "Loss after 791 iterations: 0.3876083791255951\n",
      "Loss after 792 iterations: 0.38713857531547546\n",
      "Loss after 793 iterations: 0.38667044043540955\n",
      "Loss after 794 iterations: 0.386203795671463\n",
      "Loss after 795 iterations: 0.38573870062828064\n",
      "Loss after 796 iterations: 0.3852751553058624\n",
      "Loss after 797 iterations: 0.38481318950653076\n",
      "Loss after 798 iterations: 0.3843526542186737\n",
      "Loss after 799 iterations: 0.3838936984539032\n",
      "Loss after 800 iterations: 0.38343629240989685\n",
      "Loss after 801 iterations: 0.3829803168773651\n",
      "Loss after 802 iterations: 0.38252589106559753\n",
      "Loss after 803 iterations: 0.38207289576530457\n",
      "Loss after 804 iterations: 0.381621390581131\n",
      "Loss after 805 iterations: 0.3811713457107544\n",
      "Loss after 806 iterations: 0.38072288036346436\n",
      "Loss after 807 iterations: 0.38027575612068176\n",
      "Loss after 808 iterations: 0.37983015179634094\n",
      "Loss after 809 iterations: 0.3793860375881195\n",
      "Loss after 810 iterations: 0.37894323468208313\n",
      "Loss after 811 iterations: 0.37850192189216614\n",
      "Loss after 812 iterations: 0.37806203961372375\n",
      "Loss after 813 iterations: 0.37762364745140076\n",
      "Loss after 814 iterations: 0.3771865665912628\n",
      "Loss after 815 iterations: 0.3767509460449219\n",
      "Loss after 816 iterations: 0.37631669640541077\n",
      "Loss after 817 iterations: 0.37588390707969666\n",
      "Loss after 818 iterations: 0.3754524290561676\n",
      "Loss after 819 iterations: 0.37502238154411316\n",
      "Loss after 820 iterations: 0.37459370493888855\n",
      "Loss after 821 iterations: 0.3741663694381714\n",
      "Loss after 822 iterations: 0.3737403452396393\n",
      "Loss after 823 iterations: 0.3733157813549042\n",
      "Loss after 824 iterations: 0.3728925287723541\n",
      "Loss after 825 iterations: 0.3724706470966339\n",
      "Loss after 826 iterations: 0.37205004692077637\n",
      "Loss after 827 iterations: 0.37163078784942627\n",
      "Loss after 828 iterations: 0.3712129294872284\n",
      "Loss after 829 iterations: 0.3707962930202484\n",
      "Loss after 830 iterations: 0.3703809976577759\n",
      "Loss after 831 iterations: 0.36996695399284363\n",
      "Loss after 832 iterations: 0.3695543110370636\n",
      "Loss after 833 iterations: 0.369143009185791\n",
      "Loss after 834 iterations: 0.36873289942741394\n",
      "Loss after 835 iterations: 0.36832407116889954\n",
      "Loss after 836 iterations: 0.3679165542125702\n",
      "Loss after 837 iterations: 0.3675103187561035\n",
      "Loss after 838 iterations: 0.3671053349971771\n",
      "Loss after 839 iterations: 0.366701602935791\n",
      "Loss after 840 iterations: 0.3662990629673004\n",
      "Loss after 841 iterations: 0.3658978044986725\n",
      "Loss after 842 iterations: 0.3654978275299072\n",
      "Loss after 843 iterations: 0.3650991916656494\n",
      "Loss after 844 iterations: 0.36470162868499756\n",
      "Loss after 845 iterations: 0.36430537700653076\n",
      "Loss after 846 iterations: 0.3639103174209595\n",
      "Loss after 847 iterations: 0.3635164797306061\n",
      "Loss after 848 iterations: 0.3631238043308258\n",
      "Loss after 849 iterations: 0.3627323806285858\n",
      "Loss after 850 iterations: 0.36234214901924133\n",
      "Loss after 851 iterations: 0.36195310950279236\n",
      "Loss after 852 iterations: 0.3615652322769165\n",
      "Loss after 853 iterations: 0.36117860674858093\n",
      "Loss after 854 iterations: 0.3607930839061737\n",
      "Loss after 855 iterations: 0.36040881276130676\n",
      "Loss after 856 iterations: 0.3600257635116577\n",
      "Loss after 857 iterations: 0.3596436679363251\n",
      "Loss after 858 iterations: 0.3592628538608551\n",
      "Loss after 859 iterations: 0.35888317227363586\n",
      "Loss after 860 iterations: 0.3585047423839569\n",
      "Loss after 861 iterations: 0.358127236366272\n",
      "Loss after 862 iterations: 0.3577510416507721\n",
      "Loss after 863 iterations: 0.35737594962120056\n",
      "Loss after 864 iterations: 0.35700199007987976\n",
      "Loss after 865 iterations: 0.3566291332244873\n",
      "Loss after 866 iterations: 0.3562573492527008\n",
      "Loss after 867 iterations: 0.35588669776916504\n",
      "Loss after 868 iterations: 0.35551717877388\n",
      "Loss after 869 iterations: 0.3551488220691681\n",
      "Loss after 870 iterations: 0.35478150844573975\n",
      "Loss after 871 iterations: 0.35441529750823975\n",
      "Loss after 872 iterations: 0.3540501594543457\n",
      "Loss after 873 iterations: 0.3536861836910248\n",
      "Loss after 874 iterations: 0.3533231317996979\n",
      "Loss after 875 iterations: 0.35296130180358887\n",
      "Loss after 876 iterations: 0.35260045528411865\n",
      "Loss after 877 iterations: 0.3522406816482544\n",
      "Loss after 878 iterations: 0.35188207030296326\n",
      "Loss after 879 iterations: 0.35152432322502136\n",
      "Loss after 880 iterations: 0.35116779804229736\n",
      "Loss after 881 iterations: 0.35081228613853455\n",
      "Loss after 882 iterations: 0.3504577875137329\n",
      "Loss after 883 iterations: 0.35010430216789246\n",
      "Loss after 884 iterations: 0.3497518301010132\n",
      "Loss after 885 iterations: 0.34940052032470703\n",
      "Loss after 886 iterations: 0.3490501642227173\n",
      "Loss after 887 iterations: 0.34870079159736633\n",
      "Loss after 888 iterations: 0.34835243225097656\n",
      "Loss after 889 iterations: 0.3480052053928375\n",
      "Loss after 890 iterations: 0.3476588726043701\n",
      "Loss after 891 iterations: 0.3473135530948639\n",
      "Loss after 892 iterations: 0.34696927666664124\n",
      "Loss after 893 iterations: 0.3466259241104126\n",
      "Loss after 894 iterations: 0.34628358483314514\n",
      "Loss after 895 iterations: 0.34594228863716125\n",
      "Loss after 896 iterations: 0.34560200572013855\n",
      "Loss after 897 iterations: 0.3452626168727875\n",
      "Loss after 898 iterations: 0.3449242413043976\n",
      "Loss after 899 iterations: 0.3445868492126465\n",
      "Loss after 900 iterations: 0.34425032138824463\n",
      "Loss after 901 iterations: 0.34391483664512634\n",
      "Loss after 902 iterations: 0.34358033537864685\n",
      "Loss after 903 iterations: 0.343246728181839\n",
      "Loss after 904 iterations: 0.3429141044616699\n",
      "Loss after 905 iterations: 0.34258246421813965\n",
      "Loss after 906 iterations: 0.3422516882419586\n",
      "Loss after 907 iterations: 0.34192195534706116\n",
      "Loss after 908 iterations: 0.34159305691719055\n",
      "Loss after 909 iterations: 0.34126517176628113\n",
      "Loss after 910 iterations: 0.34093809127807617\n",
      "Loss after 911 iterations: 0.3406120240688324\n",
      "Loss after 912 iterations: 0.34028685092926025\n",
      "Loss after 913 iterations: 0.33996257185935974\n",
      "Loss after 914 iterations: 0.33963921666145325\n",
      "Loss after 915 iterations: 0.33931684494018555\n",
      "Loss after 916 iterations: 0.3389953076839447\n",
      "Loss after 917 iterations: 0.3386746942996979\n",
      "Loss after 918 iterations: 0.3383549153804779\n",
      "Loss after 919 iterations: 0.33803609013557434\n",
      "Loss after 920 iterations: 0.33771812915802\n",
      "Loss after 921 iterations: 0.3374011218547821\n",
      "Loss after 922 iterations: 0.33708497881889343\n",
      "Loss after 923 iterations: 0.336769700050354\n",
      "Loss after 924 iterations: 0.33645522594451904\n",
      "Loss after 925 iterations: 0.3361416757106781\n",
      "Loss after 926 iterations: 0.3358290195465088\n",
      "Loss after 927 iterations: 0.3355172574520111\n",
      "Loss after 928 iterations: 0.3352062702178955\n",
      "Loss after 929 iterations: 0.3348962068557739\n",
      "Loss after 930 iterations: 0.3345869779586792\n",
      "Loss after 931 iterations: 0.33427858352661133\n",
      "Loss after 932 iterations: 0.3339710533618927\n",
      "Loss after 933 iterations: 0.33366429805755615\n",
      "Loss after 934 iterations: 0.33335843682289124\n",
      "Loss after 935 iterations: 0.33305343985557556\n",
      "Loss after 936 iterations: 0.33274924755096436\n",
      "Loss after 937 iterations: 0.3324459493160248\n",
      "Loss after 938 iterations: 0.3321433961391449\n",
      "Loss after 939 iterations: 0.33184167742729187\n",
      "Loss after 940 iterations: 0.3315407335758209\n",
      "Loss after 941 iterations: 0.3312407433986664\n",
      "Loss after 942 iterations: 0.33094146847724915\n",
      "Loss after 943 iterations: 0.330642968416214\n",
      "Loss after 944 iterations: 0.33034536242485046\n",
      "Loss after 945 iterations: 0.3300485908985138\n",
      "Loss after 946 iterations: 0.32975253462791443\n",
      "Loss after 947 iterations: 0.32945728302001953\n",
      "Loss after 948 iterations: 0.3291628658771515\n",
      "Loss after 949 iterations: 0.3288692235946655\n",
      "Loss after 950 iterations: 0.32857638597488403\n",
      "Loss after 951 iterations: 0.3282843232154846\n",
      "Loss after 952 iterations: 0.3279930651187897\n",
      "Loss after 953 iterations: 0.32770249247550964\n",
      "Loss after 954 iterations: 0.32741278409957886\n",
      "Loss after 955 iterations: 0.32712382078170776\n",
      "Loss after 956 iterations: 0.32683560252189636\n",
      "Loss after 957 iterations: 0.32654818892478943\n",
      "Loss after 958 iterations: 0.32626160979270935\n",
      "Loss after 959 iterations: 0.3259756863117218\n",
      "Loss after 960 iterations: 0.32569053769111633\n",
      "Loss after 961 iterations: 0.32540616393089294\n",
      "Loss after 962 iterations: 0.32512253522872925\n",
      "Loss after 963 iterations: 0.32483968138694763\n",
      "Loss after 964 iterations: 0.32455751299858093\n",
      "Loss after 965 iterations: 0.3242761194705963\n",
      "Loss after 966 iterations: 0.32399553060531616\n",
      "Loss after 967 iterations: 0.32371556758880615\n",
      "Loss after 968 iterations: 0.3234364092350006\n",
      "Loss after 969 iterations: 0.3231579661369324\n",
      "Loss after 970 iterations: 0.3228802978992462\n",
      "Loss after 971 iterations: 0.3226032853126526\n",
      "Loss after 972 iterations: 0.32232704758644104\n",
      "Loss after 973 iterations: 0.3220514953136444\n",
      "Loss after 974 iterations: 0.32177668809890747\n",
      "Loss after 975 iterations: 0.32150253653526306\n",
      "Loss after 976 iterations: 0.3212291896343231\n",
      "Loss after 977 iterations: 0.3209564983844757\n",
      "Loss after 978 iterations: 0.3206844925880432\n",
      "Loss after 979 iterations: 0.3204132318496704\n",
      "Loss after 980 iterations: 0.3201427161693573\n",
      "Loss after 981 iterations: 0.3198728561401367\n",
      "Loss after 982 iterations: 0.3196036219596863\n",
      "Loss after 983 iterations: 0.31933510303497314\n",
      "Loss after 984 iterations: 0.3190673589706421\n",
      "Loss after 985 iterations: 0.31880027055740356\n",
      "Loss after 986 iterations: 0.31853383779525757\n",
      "Loss after 987 iterations: 0.3182680904865265\n",
      "Loss after 988 iterations: 0.3180030882358551\n",
      "Loss after 989 iterations: 0.31773871183395386\n",
      "Loss after 990 iterations: 0.3174750506877899\n",
      "Loss after 991 iterations: 0.31721195578575134\n",
      "Loss after 992 iterations: 0.31694963574409485\n",
      "Loss after 993 iterations: 0.3166879117488861\n",
      "Loss after 994 iterations: 0.3164269030094147\n",
      "Loss after 995 iterations: 0.31616660952568054\n",
      "Loss after 996 iterations: 0.3159068524837494\n",
      "Loss after 997 iterations: 0.31564781069755554\n",
      "Loss after 998 iterations: 0.315389484167099\n",
      "Loss after 999 iterations: 0.315131813287735\n"
     ]
    }
   ],
   "source": [
    "# compile all forward backward into loop of iterations\n",
    "num_iterations = 1000\n",
    "for i in range(num_iterations):\n",
    "    # forward pass\n",
    "    logits = input @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    loss = -probs[torch.arange(len(input)), y].log().mean()\n",
    "    print(f\"Loss after {i} iterations: {loss}\")\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -learning_rate*W.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we trained the NN on a small number of examples (12) till now\n",
    "- lets train on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for name in names:\n",
    "    name = '..' + name + '.'\n",
    "    for c1, c2, c3 in zip(name, name[1:], name[2:]):\n",
    "        # print(c1,c2,c3)\n",
    "        X.append([ctoi[c1],ctoi[c2]+vocab_size])\n",
    "        y.append(ctoi[c3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.zeros((len(X), 2*vocab_size))\n",
    "for i in range(len(X)):\n",
    "    input[i][X[i][0]] = 0.5\n",
    "    input[i][X[i][1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights and hyperparameters\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((2*vocab_size, vocab_size), generator=g, requires_grad=True)\n",
    "# hyperparameters\n",
    "learning_rate = 10\n",
    "num_iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3369383811950684\n",
      "2.3369383811950684\n",
      "2.3369383811950684\n",
      "2.3369381427764893\n",
      "2.3369381427764893\n",
      "2.336937665939331\n",
      "2.336937427520752\n",
      "2.336937189102173\n",
      "2.3369369506835938\n",
      "2.3369369506835938\n",
      "2.3369367122650146\n",
      "2.3369364738464355\n",
      "2.3369364738464355\n",
      "2.3369359970092773\n",
      "2.3369359970092773\n",
      "2.3369359970092773\n",
      "2.336935520172119\n",
      "2.33693528175354\n",
      "2.33693528175354\n",
      "2.33693528175354\n",
      "2.336935043334961\n",
      "2.336935043334961\n",
      "2.3369345664978027\n",
      "2.3369345664978027\n",
      "2.3369340896606445\n",
      "2.3369338512420654\n",
      "2.3369338512420654\n",
      "2.3369336128234863\n",
      "2.3369333744049072\n",
      "2.3369333744049072\n",
      "2.336933135986328\n",
      "2.336932897567749\n",
      "2.336932897567749\n",
      "2.336932897567749\n",
      "2.3369321823120117\n",
      "2.3369321823120117\n",
      "2.3369319438934326\n",
      "2.3369317054748535\n",
      "2.3369314670562744\n",
      "2.3369314670562744\n",
      "2.3369312286376953\n",
      "2.3369312286376953\n",
      "2.3369312286376953\n",
      "2.336930513381958\n",
      "2.336930513381958\n",
      "2.336930274963379\n",
      "2.3369300365448\n",
      "2.3369300365448\n",
      "2.3369300365448\n",
      "2.3369295597076416\n",
      "2.3369295597076416\n",
      "2.3369295597076416\n",
      "2.3369293212890625\n",
      "2.3369290828704834\n",
      "2.3369290828704834\n",
      "2.336928367614746\n",
      "2.336928367614746\n",
      "2.336928129196167\n",
      "2.336928129196167\n",
      "2.336927890777588\n",
      "2.336927652359009\n",
      "2.336927652359009\n",
      "2.3369274139404297\n",
      "2.3369274139404297\n",
      "2.3369274139404297\n",
      "2.3369266986846924\n",
      "2.3369266986846924\n",
      "2.3369264602661133\n",
      "2.336926221847534\n",
      "2.336926221847534\n",
      "2.336925983428955\n",
      "2.336925745010376\n",
      "2.336925745010376\n",
      "2.336925506591797\n",
      "2.3369250297546387\n",
      "2.3369250297546387\n",
      "2.3369247913360596\n",
      "2.3369247913360596\n",
      "2.3369245529174805\n",
      "2.3369243144989014\n",
      "2.3369243144989014\n",
      "2.3369240760803223\n",
      "2.336923837661743\n",
      "2.336923599243164\n",
      "2.336923599243164\n",
      "2.336923599243164\n",
      "2.3369228839874268\n",
      "2.3369228839874268\n",
      "2.3369226455688477\n",
      "2.3369226455688477\n",
      "2.3369224071502686\n",
      "2.3369221687316895\n",
      "2.3369219303131104\n",
      "2.3369219303131104\n",
      "2.3369219303131104\n",
      "2.3369216918945312\n",
      "2.336921215057373\n",
      "2.336921215057373\n",
      "2.336920976638794\n",
      "2.336920738220215\n"
     ]
    }
   ],
   "source": [
    "# compile all forward backward into loop of iterations\n",
    "num_iterations = 100\n",
    "learning_rate = 20\n",
    "for i in range(num_iterations):\n",
    "    # forward pass\n",
    "    logits = input @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    loss = -probs[torch.arange(len(input)), y].log().mean()\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -learning_rate*W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ka.\n",
      "kene.\n",
      "madri.\n",
      "ava.\n",
      "gri.\n",
      "ber.\n",
      "rayino.\n",
      "urite.\n",
      "tah.\n",
      "rahemikely.\n"
     ]
    }
   ],
   "source": [
    "# use the same generator to sample from the neural network\n",
    "g = torch.Generator().manual_seed(2147483647)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caleeliynaysuveryel.\n",
      "catain.\n",
      "kamikadem.\n",
      "key.\n",
      "krim.\n",
      "elaiman.\n",
      "naplentaege.\n",
      "ny.\n",
      "cangtoyn.\n",
      "are.\n"
     ]
    }
   ],
   "source": [
    "# lets generate 10 names from scratch\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    # initialize the input bigram as '..' indicating start of the name\n",
    "    ix1 = 26 # keeps track of 2nd last character\n",
    "    ix2 = 26 # keeps track of index generated by model, use itoc[ix2] to get the corresponding character\n",
    "    while True:\n",
    "        test_input = torch.zeros((1, 2*vocab_size))\n",
    "        test_input[0][ix1] = 0.5\n",
    "        test_input[0][vocab_size+ix2] = 1\n",
    "        # W.requires_grad = False # should be set to false for inference, saves the extra memory for gradients which is not required\n",
    "        logits = test_input @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdim=True)\n",
    "        ix3 = torch.multinomial(probs, num_samples=1, replacement = True, generator=g).item() # sample 1 character from the probability distribution generated by the model using multinomial sampling with replacement [as names can have repeating characters]\n",
    "        # following statement fails if we don't use .item()!! don't forget to put it as then it outputs a tensor!\n",
    "        out.append(itoc[ix3])\n",
    "\n",
    "        # stop if '.' is generated as it is our stop token in this case\n",
    "        if ix3 == 26:\n",
    "            break\n",
    "\n",
    "        ix1 = ix2 # update ix1 to hold the next character\n",
    "        ix2 = ix3 # update ix2 to the newly generated character\n",
    "        # these two characters will be used to sample in next iteration\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
